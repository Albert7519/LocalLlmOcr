import streamlit as st
import torch
from modelscope import Qwen2_5_VLForConditionalGeneration, AutoProcessor
from PIL import Image
import io # To handle byte stream from file uploader

# --- Cached Model Loading ---
# Use st.cache_resource to load the model and processor only once
@st.cache_resource
def load_model_and_processor():
    """Loads the Qwen VL model and processor, cached by Streamlit.""" # Reformat docstring line
    print("Loading model and processor (this should run only once)...")
    try:
        model = Qwen2_5_VLForConditionalGeneration.from_pretrained(
            "Qwen/Qwen2.5-VL-7B-Instruct-AWQ",
            torch_dtype=torch.float16,
            attn_implementation="flash_attention_2", # Requires flash-attn installation
            device_map="cuda", # Load directly to GPU
        )
        # --- Add min_pixels and max_pixels ---
        min_pixels = 1024 * 28 * 28 # Encourage higher detail processing
        max_pixels = 16384 * 28 * 28 # Set an upper limit (adjust as needed)
        processor = AutoProcessor.from_pretrained(
            "Qwen/Qwen2.5-VL-7B-Instruct-AWQ",
            use_fast=True,
            min_pixels=min_pixels, # Add min_pixels
            max_pixels=max_pixels,  # Add max_pixels
            vl_high_resolution_images=True # Add high resolution flag
        )
        print(f"Model and processor loaded successfully (min_pixels={min_pixels}, max_pixels={max_pixels}, vl_high_resolution_images=True).")
        return model, processor
    except ImportError:
        st.error("Error: flash_attn package not found. Please install it: pip install flash-attn --no-build-isolation")
        return None, None
    except Exception as e:
        st.error(f"Error loading model or processor: {e}")
        return None, None

# --- Streamlit UI Setup ---
st.set_page_config(layout="wide") # Use wide layout for better display
st.title("ğŸ§¾ Qwen-VL å‘ç¥¨ OCR åº”ç”¨")

# Load model and processor using the cached function
model, processor = load_model_and_processor()

# Sidebar for inputs
with st.sidebar:
    st.header("è¾“å…¥è®¾ç½®")
    # File uploader for the receipt image
    uploaded_file = st.file_uploader("ä¸Šä¼ å‘ç¥¨å›¾ç‰‡", type=["jpg", "jpeg", "png"])

    # Text area for the prompt
    default_prompt = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å›¾åƒæ•°æ®å¤„ç†åŠ©æ‰‹ï¼Œè´Ÿè´£ä»æä¾›çš„å‘ç¥¨å›¾ç‰‡ä¸­æå–ä¿¡æ¯ï¼Œå¹¶å°†å…¶è½¬æ¢ä¸ºæ ‡å‡†åŒ–çš„CSVè¡¨æ ¼æ ¼å¼ã€‚

è¯·åˆ†æä»¥ä¸‹å›¾ç‰‡ï¼Œå¹¶æ‰§è¡Œä»¥ä¸‹ä»»åŠ¡ï¼š

1.  **è¯†åˆ«å…³é”®ä¿¡æ¯**ï¼šä»å›¾ç‰‡ä¸­æ‰¾å‡ºä»¥ä¸‹å‘ç¥¨ä¿¡æ¯ï¼š
    *   å‘ç¥¨æ—¥æœŸ
    *   é‡‘é¢
    *   å‘ç¥¨å·ç 
    *   è½¦å·
    *   ä¸Šè½¦æ—¶é—´
    *   ä¸‹è½¦æ—¶é—´

2.  **è§„èŒƒåŒ–è¾“å‡º**ï¼šå°†æå–çš„ä¿¡æ¯æ•´ç†æˆCSVæ ¼å¼ï¼Œå­—æ®µé¡ºåºå’Œæ ¼å¼è¦æ±‚å¦‚ä¸‹ï¼š
    *   å‘ç¥¨æ—¥æœŸ (æ ¼å¼: YYYY-MM-DD)
    *   é‡‘é¢ (å•ä½: å…ƒï¼Œä¿ç•™ä¸¤ä½å°æ•°)
    *   å‘ç¥¨å·ç 
    *   è½¦å· (æ ¼å¼: XX-XXXXXX)
    *   ä¸Šè½¦æ—¶é—´ (æ ¼å¼: HH:MM)
    *   ä¸‹è½¦æ—¶é—´ (æ ¼å¼: HH:MM)

3.  **å¤„ç†ç¼ºå¤±å€¼**ï¼š
    *   å¦‚æœå›¾ç‰‡ä¸­æ‰¾ä¸åˆ°å‘ç¥¨æ—¥æœŸï¼Œè¯·åœ¨CSVå¯¹åº”ä½ç½®å¡«å†™ "NULL"ã€‚
    *   å¦‚æœå›¾ç‰‡ä¸­æ‰¾ä¸åˆ°é‡‘é¢ï¼Œè¯·åœ¨CSVå¯¹åº”ä½ç½®å¡«å†™ "NULL"ã€‚
    *   å¦‚æœå›¾ç‰‡ä¸­æ‰¾ä¸åˆ°å‘ç¥¨å·ç ï¼Œè¯·åœ¨CSVå¯¹åº”ä½ç½®å¡«å†™ "NULL"ã€‚
    *   å¦‚æœå›¾ç‰‡ä¸­æ‰¾ä¸åˆ°è½¦å·ï¼Œè¯·åœ¨CSVå¯¹åº”ä½ç½®å¡«å†™ "NULL"ã€‚
    *   å¦‚æœå›¾ç‰‡ä¸­æ‰¾ä¸åˆ°ä¸Šè½¦æ—¶é—´ï¼Œè¯·åœ¨CSVå¯¹åº”ä½ç½®å¡«å†™ "NULL"ã€‚
    *   å¦‚æœå›¾ç‰‡ä¸­æ‰¾ä¸åˆ°ä¸‹è½¦æ—¶é—´ï¼Œè¯·åœ¨CSVå¯¹åº”ä½ç½®å¡«å†™ "NULL"ã€‚

4.  **è¾“å‡ºæ ¼å¼ç¤ºä¾‹**ï¼š
    ```csv
    å‘ç¥¨æ—¥æœŸ,é‡‘é¢,å‘ç¥¨å·ç ,è½¦å·,ä¸Šè½¦æ—¶é—´,ä¸‹è½¦æ—¶é—´
    YYYY-MM-DD,XX.XX,XXXXXXXX,XX-XXXXXX,HH:MM,HH:MM
    ```
    è¯·åªè¾“å‡ºCSVå†…å®¹ï¼Œä¸è¦åŒ…å«è¡¨å¤´ã€‚

5. **æ³¨æ„äº‹é¡¹**ï¼š
    å¦‚æœå­˜åœ¨ç¨å‰ä»·æ ¼å’Œç¨åä»·æ ¼ï¼Œåªç»Ÿè®¡ç¨åä»·æ ¼ã€‚

è¯·æ ¹æ®ä»¥ä¸Šè§„åˆ™å¤„ç†å›¾ç‰‡ä¸­çš„å‘ç¥¨ä¿¡æ¯ã€‚"""

    prompt = st.text_area("æå–æŒ‡ä»¤ (Prompt)", value=default_prompt, height=150)

    # Button to trigger OCR
    submit_button = st.button("å¼€å§‹æå–ä¿¡æ¯")

# Main area for displaying image and results
col1, col2 = st.columns(2)

with col1:
    st.subheader("ä¸Šä¼ çš„å›¾ç‰‡")
    if uploaded_file is not None:
        # Display the uploaded image
        try:
            raw_image = Image.open(uploaded_file).convert('RGB')
            st.image(raw_image, caption="Uploaded Receipt", use_container_width=True)
        except Exception as e:
            st.error(f"æ— æ³•åŠ è½½å›¾ç‰‡: {e}")
            raw_image = None # Ensure raw_image is None if loading fails
    else:
        st.info("è¯·åœ¨å·¦ä¾§ä¸Šä¼ ä¸€å¼ å‘ç¥¨å›¾ç‰‡ã€‚")
        raw_image = None

with col2:
    st.subheader("æå–ç»“æœ")
    # Placeholder for the results
    result_placeholder = st.empty()
    result_placeholder.info("ç‚¹å‡»â€œå¼€å§‹æå–ä¿¡æ¯â€æŒ‰é’®åï¼Œç»“æœå°†æ˜¾ç¤ºåœ¨è¿™é‡Œã€‚")

# --- OCR Processing Logic ---
if submit_button and raw_image is not None and model is not None and processor is not None:
    with st.spinner("æ­£åœ¨å¤„ç†å›¾ç‰‡å¹¶æå–ä¿¡æ¯..."):
        try:
            # Structure the input messages (similar to ocr_receipt.py)
            messages = [
                {
                    "role": "user",
                    "content": [
                        {"type": "text", "text": prompt},
                        {"type": "image"}, # Placeholder for the image
                    ],
                }
            ]

            # Prepare inputs using the processor
            # Note: We pass the raw PIL image directly
            text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
            inputs = processor(
                text=[text],
                images=[raw_image], # Pass the loaded PIL Image object
                padding=True,
                return_tensors="pt",
            ).to("cuda")

            # Generate response
            with torch.no_grad():
                generated_ids = model.generate(
                    **inputs,
                    max_new_tokens=1024,
                    do_sample=False # Use greedy decoding
                )

            # Decode the output
            generated_ids_trimmed = [
                out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)
            ]
            output_text = processor.batch_decode(
                generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False
            )

            # Display the result
            if output_text:
                result_placeholder.markdown(output_text[0]) # Use markdown for better formatting if needed
            else:
                result_placeholder.warning("æ¨¡å‹æ²¡æœ‰ç”Ÿæˆä»»ä½•æ–‡æœ¬ã€‚")

        except Exception as e:
            result_placeholder.error(f"å¤„ç†è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
elif submit_button and raw_image is None:
    st.sidebar.warning("è¯·å…ˆä¸Šä¼ ä¸€å¼ å›¾ç‰‡ã€‚")
elif submit_button and (model is None or processor is None):
     st.sidebar.error("æ¨¡å‹æˆ–å¤„ç†å™¨æœªèƒ½æˆåŠŸåŠ è½½ï¼Œæ— æ³•è¿›è¡Œå¤„ç†ã€‚")
